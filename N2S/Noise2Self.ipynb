{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45ef7d54-525f-401c-a090-88abd57777a1",
   "metadata": {},
   "source": [
    "# Noise2Self Denoising\n",
    "### Versions\n",
    "| Run    | Model | Epochs | Learning Rate | `masker_width` | `batch` | Patchsize | Avg. PSNR | Avg. SSIM | Avg. PIQE | Avg. BRISQUE | Avg. NIQE |\n",
    "| :----- | :---: | :----: | :-----------: | :------------: | :-----: | :-------: | :-------: | :-------: | :-------: | :----------: | :-------: |\n",
    "| **v0.0** |   -   |  -  |  -    | - | - |  -  |   -   |    -   |`59.89`|`62.16`|`9.44` |\n",
    "| **v1.0** | U-Net | 100 | 0.001 | 8 | 4 | 512 | 44.28 | 0.9875 | 41.52 | 72.77 | 12.08 |\n",
    "| **v1.1** |       |        |               |                |         |           |           |           |           |              |           |\n",
    "| **v1.2** |       |        |               |                |         |           |           |           |           |              |           |\n",
    "\n",
    "## New Methodology Overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df855ed5-e44c-4d66-9e0a-a20d45ff9610",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "This section imports all the necessary libraries for the project, remains the same but with `torchvision.transforms` for data augmentation and changed the model import from `DnCNN` to `Unet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa75634f-ad98-4e32-93aa-1094003f3cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "CUDA available: True\n",
      "CUDA device count: 1\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce RTX 3070\n"
     ]
    }
   ],
   "source": [
    "# --- Core and Data Handling Libraries ---\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# --- PyTorch Libraries ---\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import Adam\n",
    "from torchvision import transforms\n",
    "\n",
    "# --- Image Processing and Data Science Utilities ---\n",
    "from skimage.metrics import peak_signal_noise_ratio as compare_psnr\n",
    "from skimage.metrics import structural_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision.utils import save_image\n",
    "import pyiqa\n",
    "\n",
    "# --- Import from the noise2self repository ---\n",
    "from mask import Masker\n",
    "from models.unet import Unet\n",
    "from models.dncnn import DnCNN\n",
    "from util import show\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "print(\"Current device:\", torch.cuda.current_device())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "956ded2c-20de-4b79-beb2-87599dfc590a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration set.\n",
      "Image data directory: C:/Users/emanu/OneDrive - University of Cape Town/EEE4022S/Data/Final/Train_Val_512x512\n"
     ]
    }
   ],
   "source": [
    "class TrainingConfig:\n",
    "    # --- Data and Augmentation ---\n",
    "    val_split_ratio = 0.15\n",
    "\n",
    "    # --- Model and Training Hyperparameters ---\n",
    "    model = 1 # 0 = DnCNN, 1 = Unet\n",
    "    n_epoch = 50\n",
    "    n_channel = 1  # Set to 1 for grayscale X-ray images\n",
    "    num_layers = 8 # For DnCNN\n",
    "    lr = 0.001\n",
    "    batchsize = 4\n",
    "    patchsize = 512  # Can be 512, 1024, or 2048\n",
    "    masker_width = 8\n",
    "\n",
    "    # --- Paths ---\n",
    "    save_model_path = 'C:/Users/emanu/OneDrive - University of Cape Town/EEE4022S/Data/Final/N2N/Models'          # Base path to save model checkpoints\n",
    "    save_losses_path = 'C:/Users/emanu/OneDrive - University of Cape Town/EEE4022S/Data/Final/N2N/Losses'\n",
    "    save_results_path = 'C:/Users/emanu/OneDrive - University of Cape Town/EEE4022S/Data/Final/N2N/Results'\n",
    "\n",
    "    def get_hyperparameter_string(self):\n",
    "        model_str = \"Unet\" if self.model == 1 else f\"DnCNN{self.num_layers}\"\n",
    "        return (f\"{model_str}_ep{self.n_epoch}_lr{self.lr}_\"\n",
    "                f\"b{self.batchsize}_p{self.patchsize}_mw{self.masker_width}\")\n",
    "        \n",
    "    if patchsize == 512:\n",
    "        img_dir = 'C:/Users/emanu/OneDrive - University of Cape Town/EEE4022S/Data/Final/Train_Val_512x512'\n",
    "        test_img_dir = 'C:/Users/emanu/OneDrive - University of Cape Town/EEE4022S/Data/Final/Test_512x512'\n",
    "    elif patchsize == 1024:\n",
    "        img_dir = 'C:/Users/emanu/OneDrive - University of Cape Town/EEE4022S/Data/Final/Train_Val_1024x1024'\n",
    "        test_img_dir = 'C:/Users/emanu/OneDrive - University of Cape Town/EEE4022S/Data/Final/Test_1024x1024'\n",
    "    elif patchsize == 2048:\n",
    "        img_dir = 'C:/Users/emanu/OneDrive - University of Cape Town/EEE4022S/Data/Final/Train_Val_2048x2048'\n",
    "        test_img_dir = 'C:/Users/emanu/OneDrive - University of Cape Town/EEE4022S/Data/Final/Test_2048x2048'\n",
    "\n",
    "opt = TrainingConfig()\n",
    "print(\"Configuration set.\")\n",
    "print(f\"Image data directory: {opt.img_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ba98f1-a809-4eb3-ae41-fc9ce9453af4",
   "metadata": {},
   "source": [
    "## 2. Data Loading (from Offline Augmented Dataset)\n",
    "\n",
    "Now that we have created a large, pre-augmented training set and a separate test set, this section will load that data.\n",
    "\n",
    "-   **Simplified Transforms**: Since all augmentations have already been applied offline, the only transformation needed here is `transforms.ToTensor()`, which converts the loaded PIL images into PyTorch tensors and normalizes their pixel values to the `[0, 1]` range.\n",
    "-   **Dataset Splitting**: The code will now take the images from your new `train_val_dir`, and split them into a final training set and a validation set. The `test_dir` will be loaded separately for the final evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55bee5ec-86de-46c3-87c0-40f740c7f612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 340 total augmented train/val images.\n",
      "Found 15 total un-augmented test images.\n",
      "Initialized dataset with 289 images.\n",
      "Initialized dataset with 51 images.\n",
      "Initialized dataset with 15 images.\n",
      "\n",
      "Dataset sizes for this run:\n",
      "  Training: 289\n",
      "  Validation: 51\n",
      "  Test: 15\n"
     ]
    }
   ],
   "source": [
    "# --- Define the Dataset Class ---\n",
    "class DicomTensorDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for loading pre-processed X-ray images.\n",
    "    Applies specified transforms to each image upon loading.\n",
    "    \"\"\"\n",
    "    def __init__(self, file_paths, transform=None):\n",
    "        super(DicomTensorDataset, self).__init__()\n",
    "        self.image_files = file_paths\n",
    "        self.transform = transform\n",
    "        print(f'Initialized dataset with {len(self.image_files)} images.')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.image_files[index]\n",
    "        im = Image.open(img_path).convert('L') # Convert to grayscale\n",
    "        if self.transform:\n",
    "            im = self.transform(im)\n",
    "        return im, os.path.basename(img_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "# --- Define the simplified transformation ---\n",
    "online_transforms = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# --- Load Files from the New Offline Directories ---\n",
    "# Get all file paths from your new augmented train/val folder\n",
    "train_val_files = sorted([os.path.join(opt.img_dir, f) for f in os.listdir(opt.img_dir)])\n",
    "# Get all file paths from your new un-augmented test folder\n",
    "test_files = sorted([os.path.join(opt.test_img_dir, f) for f in os.listdir(opt.test_img_dir)])\n",
    "\n",
    "print(f\"Found {len(train_val_files)} total augmented train/val images.\")\n",
    "print(f\"Found {len(test_files)} total un-augmented test images.\")\n",
    "\n",
    "# Now, split the augmented train_val set into a final training and validation set.\n",
    "val_size = int(len(train_val_files) * opt.val_split_ratio)\n",
    "train_size = len(train_val_files) - val_size\n",
    "generator = torch.Generator().manual_seed(42) # for reproducible splits\n",
    "train_files, val_files = random_split(train_val_files, [train_size, val_size], generator=generator)\n",
    "train_files, val_files = list(train_files), list(val_files) # Convert to lists\n",
    "\n",
    "# --- Create Datasets with the simplified transform ---\n",
    "train_dataset = DicomTensorDataset(train_files, transform=online_transforms)\n",
    "val_dataset = DicomTensorDataset(val_files, transform=online_transforms)\n",
    "test_dataset = DicomTensorDataset(test_files, transform=online_transforms)\n",
    "\n",
    "print(f\"\\nDataset sizes for this run:\")\n",
    "print(f\"  Training: {len(train_dataset)}\")\n",
    "print(f\"  Validation: {len(val_dataset)}\")\n",
    "print(f\"  Test: {len(test_dataset)}\")\n",
    "\n",
    "# --- Create DataLoaders ---\n",
    "train_loader = DataLoader(dataset=train_dataset, num_workers=0, batch_size=opt.batchsize, shuffle=True, pin_memory=True, drop_last=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, num_workers=0, batch_size=1, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, num_workers=0, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42cd417f-a818-49b7-8c22-fb2bebf6fdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Visualize a Sample to Verify ---\n",
    "# print(\"\\nVisualizing a sample from the Training DataLoader...\")\n",
    "\n",
    "# # --- MODIFIED: Unpack the image batch and filename batch ---\n",
    "# try:\n",
    "#     sample_image_batch, sample_filename_batch = next(iter(train_loader))\n",
    "    \n",
    "#     print(\"Image Batch shape:\", sample_image_batch.size())\n",
    "#     print(\"Filenames in batch:\", sample_filename_batch)\n",
    "\n",
    "#     fig, axes = plt.subplots(1, min(4, opt.batchsize), figsize=(12, 3))\n",
    "#     for i in range(min(4, opt.batchsize)):\n",
    "#         # --- MODIFIED: Use sample_image_batch ---\n",
    "#         img = sample_image_batch[i].squeeze().cpu().numpy()\n",
    "#         if min(4, opt.batchsize) == 1:\n",
    "#             ax = axes\n",
    "#         else:\n",
    "#             ax = axes[i]\n",
    "#         ax.imshow(img, cmap='gray')\n",
    "#         ax.set_title(f\"Sample {i+1}\")\n",
    "#         ax.axis('off')\n",
    "#     plt.suptitle(\"Sample Training Images (Post-Transform)\")\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# except StopIteration:\n",
    "#     print(\"Training DataLoader is empty. Cannot visualize sample.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d55927c-b0d5-456c-83d1-6d1bc3d62c0f",
   "metadata": {},
   "source": [
    "## 5. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a38daff-1cd4-4ee3-9007-2b93eee7b09e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using hyperparameter string: Unet_ep50_lr0.001_b4_p512_mw8\n",
      "Models and logs will be saved to: C:/Users/emanu/OneDrive - University of Cape Town/EEE4022S/Data/Final/N2N/Models\\Unet_ep50_lr0.001_b4_p512_mw8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/50: 100%|███████████████████████████████████████████████| 72/72 [00:12<00:00,  5.63it/s, loss=1.01e-5]\n",
      "Validation Epoch 1/50: 100%|█████████████████████████████████████████| 51/51 [00:02<00:00, 19.08it/s, val_loss=9.09e-6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50] | Avg Train Loss: 0.000239 | Avg Val Loss: 0.000013\n",
      "   -> New best model saved to C:/Users/emanu/OneDrive - University of Cape Town/EEE4022S/Data/Final/N2N/Models\\Unet_ep50_lr0.001_b4_p512_mw8\\best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/50:  81%|█████████████████████████████████████▊         | 58/72 [00:12<00:02,  4.82it/s, loss=6.86e-6]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[32m     53\u001b[39m     loss.backward()\n\u001b[32m     54\u001b[39m     optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     epoch_train_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m     train_iterator.set_postfix({\u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m: loss.item()})\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# --- Validation Phase ---\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- Training Setup ---\n",
    "# The Masker is central to Noise2Self, creating masks to hide pixels during training\n",
    "masker = Masker(width=opt.masker_width, mode='interpolate')\n",
    "# Define the model used\n",
    "if opt.model == 1:\n",
    "    model = Unet(n_channel_in=opt.n_channel, n_channel_out=opt.n_channel).to(device)\n",
    "elif opt.model == 0:\n",
    "    model = DnCNN(1, num_of_layers=opt.num_layers).to(device)\n",
    "# Define the loss function\n",
    "loss_function = MSELoss()\n",
    "# Define the optimiser\n",
    "optimizer = Adam(model.parameters(), lr=opt.lr)\n",
    "# Define the scheduler\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(x * opt.n_epoch) for x in [0.2, 0.4, 0.6, 0.8]], gamma=0.5)\n",
    "\n",
    "# --- MODIFIED: Get hyperparam string for filenames ---\n",
    "hyperparam_str = opt.get_hyperparameter_string()\n",
    "print(f\"Using hyperparameter string: {hyperparam_str}\")\n",
    "\n",
    "# --- Model Saving Setup ---\n",
    "save_model_dir = os.path.join(opt.save_model_path, hyperparam_str)\n",
    "os.makedirs(save_model_dir, exist_ok=True)\n",
    "print(f\"Models and logs will be saved to: {save_model_dir}\")\n",
    "\n",
    "# --- Lists to store loss history for plotting ---\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "# --- TRAINING & VALIDATION ---\\n\",\n",
    "start_time = time.time()\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(1, opt.n_epoch + 1):\n",
    "    # --- Training Phase ---\n",
    "    model.train()\n",
    "    epoch_train_loss = 0\n",
    "    train_iterator = tqdm(train_loader, desc=f\"Training Epoch {epoch}/{opt.n_epoch}\")\n",
    "    \n",
    "    # --- MODIFIED: Unpack image and filename from loader ---\n",
    "    # We don't need the filename for training, so we use _\n",
    "    for i, (noisy_image_batch, _) in enumerate(train_iterator):\n",
    "        noisy_image = noisy_image_batch.to(device)\n",
    "\n",
    "        # Apply the Noise2Self mask\n",
    "        net_input, mask = masker.mask(noisy_image, i)\n",
    "\n",
    "        # Get the model's prediction and calculate loss\n",
    "        net_output = model(net_input)\n",
    "        loss = loss_function(net_output * mask, noisy_image * mask)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_train_loss += loss.item()\n",
    "        train_iterator.set_postfix({'loss': loss.item()})\n",
    "\n",
    "    # --- Validation Phase ---\n",
    "    model.eval()\n",
    "    epoch_val_loss = 0\n",
    "    val_iterator = tqdm(val_loader, desc=f\"Validation Epoch {epoch}/{opt.n_epoch}\")\n",
    "    with torch.no_grad():\n",
    "        # --- MODIFIED: Unpack image and filename from loader ---\n",
    "        for i, (val_image_batch, _) in enumerate(val_iterator):\n",
    "            val_image = val_image_batch.to(device)\n",
    "\n",
    "            # Apply the Noise2Self mask for validation loss calculation\n",
    "            net_input_val, mask_val = masker.mask(val_image, i)\n",
    "            \n",
    "            # Get model output and calculate loss on masked pixels\n",
    "            net_output_val = model(net_input_val)\n",
    "            val_loss = loss_function(net_output_val * mask_val, val_image * mask_val)\n",
    "            \n",
    "            epoch_val_loss += val_loss.item()\n",
    "            val_iterator.set_postfix({'val_loss': val_loss.item()})\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # --- Log and Save ---\n",
    "    avg_train_loss = epoch_train_loss / len(train_loader)\n",
    "    avg_val_loss = epoch_val_loss / len(val_loader)\n",
    "    \n",
    "    train_loss_history.append(avg_train_loss)\n",
    "    val_loss_history.append(avg_val_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch}/{opt.n_epoch}] | Avg Train Loss: {avg_train_loss:.6f} | Avg Val Loss: {avg_val_loss:.6f}\")\n",
    "\n",
    "    # --- MODIFIED: Save Best Model with descriptive name ---\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_model_name = f\"best_model.pth\"\n",
    "        best_model_path = os.path.join(save_model_dir, best_model_name)\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f'   -> New best model saved to {best_model_path}')\n",
    "\n",
    "# --- Save Final Model ---\n",
    "final_model_name = f\"final_model.pth\"\n",
    "final_model_path = os.path.join(save_model_dir, final_model_name)\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f'   -> Final model saved to {final_model_path}')\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\n--- Training Finished ---\")\n",
    "print(f\"Total training time: {(end_time - start_time) / 60:.2f} minutes\")\n",
    "\n",
    "# --- MODIFIED: Save Loss History to CSV ---\n",
    "print(f\"\\nSaving loss history to CSV...\")\n",
    "losses_df = pd.DataFrame({\n",
    "    'epoch': range(1, opt.n_epoch + 1),\n",
    "    'train_loss': train_loss_history,\n",
    "    'val_loss': val_loss_history\n",
    "})\n",
    "losses_csv_name = f\"{hyperparam_str}.csv\"\n",
    "losses_csv_path = os.path.join(opt.save_losses_path, losses_csv_name)\n",
    "losses_df.to_csv(losses_csv_path, index=False)\n",
    "print(f\"Loss history saved to {losses_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0accc6-4497-4e74-9656-b91b892320f0",
   "metadata": {},
   "source": [
    "## 6. Performance Visualisation\n",
    "After training is complete, this cell generates plots of the training loss and validation PSNR over time. These graphs are crucial for understanding the model's learning process.\n",
    "\n",
    "- **Training Loss Plot**: We expect the loss to decrease over time, indicating that the model is learning.\n",
    "- **Validation PSNR Plot**: We expect the PSNR (a measure of image quality, where higher is better) to increase. If it starts to plateau or decrease, it might be a sign of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e309a22-376a-4d76-86fc-e61159a74972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Plotting the training and validation loss ---\n",
    "# plt.figure(figsize=(12, 5))\n",
    "# plt.plot(range(1, opt.n_epoch + 1), train_loss_history, label='Training Loss')\n",
    "# plt.plot(range(1, opt.n_epoch + 1), val_loss_history, label='Validation Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Training and Validation Loss Curves')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4062d0a3-eb42-46fd-92fc-8d759b0eeed1",
   "metadata": {},
   "source": [
    "## 7. Final Evaluation and Comprehensive Visualization\n",
    "This final section provides a comprehensive, unbiased evaluation of the best-performing model on the test set, which it has never seen before. This gives the most accurate measure of how the model will perform on new, real-world data.\n",
    "\n",
    "For each image in the test set, we will generate a three-part visual analysis and a detailed quantitative assessment:\n",
    "1. **Original Noisy Image**: The input image from the test set is displayed, along with its no-reference quality scores (PIQE, BRISQUE, NIQE) to establish a baseline for image quality.\n",
    "2. **Denoised Image**: The output of our trained model is shown. The title includes both reference metrics (PSNR and SSIM), which compare it to the original noisy image, and the no-reference metrics (PIQE, BRISQUE, NIQE). The goal is for the no-reference scores to be lower (better) than the original's.\n",
    "3. **Residual Image**: This image shows the difference between the original and the denoised version (`Original - Denoised`). It is a powerful diagnostic tool that visualizes exactly what the model has identified and removed as noise. Ideally, the residual should look like random noise and should not contain any discernible structures from the original object.\n",
    "\n",
    "Finally, the average scores for all metrics across the entire test set are calculated and printed to provide a summary of the model's overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7ef29f-b404-45e1-b80d-20a63d70bb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup ---\n",
    "print(\"\\n--- Starting Final Evaluation on Test Set ---\")\n",
    "# --- Create IQA metric models ---\n",
    "niqe_metric = pyiqa.create_metric('niqe', device=device)\n",
    "piqe_metric = pyiqa.create_metric('piqe', device=device)\n",
    "brisque_metric = pyiqa.create_metric('brisque', device=device)\n",
    "\n",
    "# --- Load the Best Model ---\n",
    "model_path = os.path.join(save_model_dir, \"best_model.pth\")\n",
    "if opt.model == 1:\n",
    "    model = Unet(n_channel_in=opt.n_channel, n_channel_out=opt.n_channel).to(device)\n",
    "else:\n",
    "    model = DnCNN(1, num_of_layers=opt.num_layers).to(device)\n",
    "\n",
    "state_dict = torch.load(model_path, weights_only=True)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "print(f\"Loaded best model for evaluation: {model_path}\\n\")\n",
    "\n",
    "# --- Prepare storage ---\n",
    "results_data = []\n",
    "original_images_np, denoised_images_np, residuals = [], [], []\n",
    "psnr_scores = []\n",
    "original_piqe_scores, denoised_piqe_scores = [], []\n",
    "original_brisque_scores, denoised_brisque_scores = [], []\n",
    "original_niqe_scores, denoised_niqe_scores = [], []\n",
    "\n",
    "test_iterator = tqdm(test_loader, desc=\"Evaluating Test Set\")\n",
    "\n",
    "# --- Evaluation ---\n",
    "with torch.no_grad():\n",
    "    for i, (original_image_batch, filename_batch) in enumerate(test_iterator):\n",
    "        original_image_tensor = original_image_batch.to(device)  # (1, 1, H, W)\n",
    "        filename = filename_batch[0]\n",
    "\n",
    "        # --- Model inference ---\n",
    "        denoised_image_tensor = model(original_image_tensor).clamp(0, 1)\n",
    "\n",
    "        # --- Compute IQA metrics (directly on tensors, no numpy) ---\n",
    "        original_niqe = niqe_metric(original_image_tensor).item()\n",
    "        denoised_niqe = niqe_metric(denoised_image_tensor).item()\n",
    "        original_piqe = piqe_metric(original_image_tensor).item()\n",
    "        denoised_piqe = piqe_metric(denoised_image_tensor).item()\n",
    "        original_brisque = brisque_metric(original_image_tensor).item()\n",
    "        denoised_brisque = brisque_metric(denoised_image_tensor).item()\n",
    "\n",
    "        # --- Convert to NumPy for PSNR and saving ---\n",
    "        original_image_np = original_image_tensor.cpu().numpy()[0, 0]\n",
    "        denoised_image_np = denoised_image_tensor.cpu().numpy()[0, 0]\n",
    "        residual_np = original_image_np - denoised_image_np\n",
    "\n",
    "        # --- Metrics ---\n",
    "        psnr = compare_psnr(original_image_np, denoised_image_np)\n",
    "\n",
    "        # --- Store all results ---\n",
    "        original_images_np.append(original_image_np)\n",
    "        denoised_images_np.append(denoised_image_np)\n",
    "        residuals.append(residual_np)\n",
    "\n",
    "        psnr_scores.append(psnr)\n",
    "        original_niqe_scores.append(original_niqe)\n",
    "        denoised_niqe_scores.append(denoised_niqe)\n",
    "        original_piqe_scores.append(original_piqe)\n",
    "        denoised_piqe_scores.append(denoised_piqe)\n",
    "        original_brisque_scores.append(original_brisque)\n",
    "        denoised_brisque_scores.append(denoised_brisque)\n",
    "\n",
    "        results_data.append({\n",
    "            'filename': filename,\n",
    "            'psnr': psnr,\n",
    "            'piqe': denoised_piqe,\n",
    "            'original_piqe': original_piqe,\n",
    "            'brisque': denoised_brisque,\n",
    "            'original_brisque': original_brisque,\n",
    "            'niqe': denoised_niqe,\n",
    "            'original_niqe': original_niqe\n",
    "        })\n",
    "\n",
    "# --- Save per-image metrics to CSV ---\n",
    "save_results_dir = os.path.join(opt.save_results_path, opt.get_hyperparameter_string())\n",
    "os.makedirs(save_results_dir, exist_ok=True)\n",
    "csv_path = os.path.join(save_results_dir, \"results.csv\")\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "results_df = results_df[['filename', 'psnr', 'piqe', 'original_piqe',\n",
    "                         'brisque', 'original_brisque', 'niqe', 'original_niqe']]\n",
    "results_df.to_csv(csv_path, index=False, float_format='%.4f')\n",
    "print(f\"Metrics saved to {csv_path}\")\n",
    "\n",
    "# --- Save average metrics ---\n",
    "avg_metrics = {\n",
    "    'run_name' : hyperparam_str,\n",
    "    'avg_psnr': np.mean(psnr_scores),\n",
    "    'avg_piqe': np.mean(denoised_piqe_scores),\n",
    "    'avg_original_piqe': np.mean(original_piqe_scores),\n",
    "    'avg_brisque': np.mean(denoised_brisque_scores),\n",
    "    'avg_original_brisque': np.mean(original_brisque_scores),\n",
    "    'avg_niqe': np.mean(denoised_niqe_scores),\n",
    "    'avg_original_niqe': np.mean(original_niqe_scores),\n",
    "}\n",
    "\n",
    "# --- Append to or create general CSV ---\n",
    "all_runs_csv = os.path.join(opt.save_results_path, \"avg_results.csv\")\n",
    "\n",
    "if os.path.exists(all_runs_csv):\n",
    "    df_all = pd.read_csv(all_runs_csv)\n",
    "    df_all = pd.concat([df_all, pd.DataFrame([avg_metrics])], ignore_index=True)\n",
    "else:\n",
    "    df_all = pd.DataFrame([avg_metrics])\n",
    "\n",
    "df_all.to_csv(all_runs_csv, index=False)\n",
    "print(f\"\\nAverage results appended to: {all_runs_csv}\")\n",
    "\n",
    "# --- Save denoised images and residuals ---\n",
    "denoised_dir = os.path.join(save_results_dir, \"denoised_images\")\n",
    "residual_dir = os.path.join(save_results_dir, \"residuals\")\n",
    "os.makedirs(denoised_dir, exist_ok=True)\n",
    "os.makedirs(residual_dir, exist_ok=True)\n",
    "\n",
    "for i, filename in enumerate(results_df['filename']):\n",
    "    denoised_uint8 = (np.clip(denoised_images_np[i], 0, 1) * 255).astype(np.uint8)\n",
    "    residual_display = (residuals[i] - residuals[i].min()) / (np.ptp(residuals[i]) + 1e-8)\n",
    "    residual_uint8 = (residual_display * 255).astype(np.uint8)\n",
    "\n",
    "    Image.fromarray(denoised_uint8).save(os.path.join(denoised_dir, f\"{filename}.png\"))\n",
    "    Image.fromarray(residual_uint8).save(os.path.join(residual_dir, f\"{filename}_residual.png\"))\n",
    "\n",
    "print(\"All denoised images and residuals saved successfully.\")\n",
    "print(\"------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b771274a-3edc-4f95-bce4-1ed11dc18284",
   "metadata": {},
   "source": [
    "# Automated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb543697-0169-4eea-bdab-6f55a045611b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (n2s)",
   "language": "python",
   "name": "n2s"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
